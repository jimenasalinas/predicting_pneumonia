{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Second Checkpoint\n",
        "### Predicting Pneumonia from X-Ray image\n",
        "\n",
        "Jimena Salinas Valdespino, Santiago Segovia Baquero, Stephania Tello Zamudio, Ivanna RodrÃ­guez Lobo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VW64heyUiHFn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn # basic building block for neural neteorks\n",
        "import torch.nn.functional as F # import convolution functions like Relu\n",
        "import torch.optim as optim # optimzer\n",
        "\n",
        "from torch.utils import data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "from torchvision.io import read_image\n",
        "\n",
        "from sklearn import metrics"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a Pytorch Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "IeESUPd2i0Qc",
        "outputId": "8d92bf29-f9be-4040-c44c-bb0001b564a6"
      },
      "outputs": [],
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir_path, resize=False, transform=None):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            - csv_file (str): file path to the csv file\n",
        "            - img_dir_path: directory path to the images\n",
        "            - transform: Compose (a PyTorch Class) that strings together several\n",
        "              transform functions (e.g. data augmentation steps)\n",
        "        \"\"\"\n",
        "        self.img_labels = pd.read_csv(csv_file, skiprows=1, header=None)\n",
        "        self.img_dir = img_dir_path\n",
        "        self.transform = transform\n",
        "        self.resize = resize\n",
        "        self.dimensions = self.get_dimensions()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns: (int) length of your dataset\n",
        "        \"\"\"\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Loads and returns your sample (the image and the label) at the\n",
        "        specified index\n",
        "\n",
        "        Parameter: idx (int): index of interest\n",
        "\n",
        "        Returns: image, label\n",
        "        \"\"\"\n",
        "        img_path =  os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "        \n",
        "        # read the image\n",
        "        image = read_image(img_path)\n",
        "\n",
        "        # get the label\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "\n",
        "        # apply transformations to image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "    \n",
        "    def get_dimensions(self):\n",
        "        \"\"\"\n",
        "        This method creates a dictionary with the unique combinations of heightxwidth\n",
        "        for each image in the dataset.\n",
        "\n",
        "        returns a dictionary with dimensions as keys and the number of images\n",
        "            with that dimension as values\n",
        "        \"\"\"\n",
        "        dimensions = {}\n",
        "        for index in range(len(self.img_labels)):\n",
        "            image = self[index][0]\n",
        "            if self.resize:\n",
        "                image = self.resize_image(image)\n",
        "            _, height, width = image.shape\n",
        "            dimensions[(height,width)] = dimensions.get((height,width),0) + 1\n",
        "\n",
        "        return dimensions\n",
        "    \n",
        "    def resize_image(self,image):\n",
        "        \"\"\"\n",
        "        If the resize parameter==True, then all the images are\n",
        "        converted to a 150x150 size.\n",
        "\n",
        "        returns the resized image\n",
        "        \"\"\"\n",
        "        transform = T.Resize((150,150))\n",
        "        \n",
        "        return transform(image)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once we implemented the Dataset class, we create one object per Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data = CustomImageDataset(csv_file = '../data/data_train.csv',\n",
        "                                img_dir_path = '../data')\n",
        "\n",
        "val_data = CustomImageDataset(csv_file = '../data/data_val.csv',\n",
        "                              img_dir_path = '../data')\n",
        "\n",
        "test_data = CustomImageDataset(csv_file = '../data/data_test.csv',\n",
        "                               img_dir_path = '../data')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The labels for each one of the datasets are the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1    3875\n",
              "0    1341\n",
              "Name: 1, dtype: int64"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train dataset\n",
        "train_data.img_labels.iloc[:,1].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    8\n",
              "1    8\n",
              "Name: 1, dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Validation dataset\n",
        "val_data.img_labels.iloc[:,1].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1    390\n",
              "0    234\n",
              "Name: 1, dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test dataset\n",
        "test_data.img_labels.iloc[:,1].value_counts()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inside our CustomImageDataset class we defined a method to compute a count of each height x width combination in our data set. The cells below display our results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4366"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.dimensions\n",
        "len(train_data.dimensions)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are 4,366 unique height x width combinations in our training data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "598"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data.dimensions\n",
        "len(test_data.dimensions)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are 598 unique height x width combinations in our testing data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_data.dimensions\n",
        "len(val_data.dimensions)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are 16 unique height x width combinations in our validation data set."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given that there are different sizes, and some very large images, we want to standarize the size of all images. We do this by passing a `resize` boolean parameter to our `CustomImageDataset` class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/stello/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/home/stello/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/home/stello/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "train_data = CustomImageDataset(csv_file = '../data/data_train.csv',\n",
        "                                img_dir_path = '../data',\n",
        "                                resize=True)\n",
        "\n",
        "val_data = CustomImageDataset(csv_file = '../data/data_val.csv',\n",
        "                              img_dir_path = '../data',\n",
        "                              resize=True)\n",
        "\n",
        "test_data = CustomImageDataset(csv_file = '../data/data_test.csv',\n",
        "                               img_dir_path = '../data',\n",
        "                               resize=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a check, we can call on the `dictionary` attribute for each one of our datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{(150, 150): 5216}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{(150, 150): 16}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_data.dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{(150, 150): 624}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data.dimensions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Image Augmentation\n",
        "\n",
        "In order to avoid overfitting, we need to do image augmentation for our training\n",
        "dataset. We do this below. We decided to augment the images following some \n",
        "examples of people who worked with this dataset in Kaggle."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The transformations we used are the following: rotate the image by 30 degrees, zoom into the image by 20%, flip the image horizontally, increase the image's sharpness, and change the color depth of the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_transform = T.Compose([\n",
        "    T.RandomRotation(30),\n",
        "    T.RandomResizedCrop(size=(150, 150), scale=(0.8, 1.2)),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomAdjustSharpness(sharpness_factor=2),\n",
        "    T.RandomPosterize(bits=4),\n",
        "    T.Grayscale(1)\n",
        "])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then apply the above transformations to our training dataset below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/stello/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Apply the transforms to the training and validation dataset\n",
        "train_data = CustomImageDataset(csv_file = '../data/data_train.csv',\n",
        "                                img_dir_path = '../data',\n",
        "                                resize=True,\n",
        "                                transform=train_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/stello/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "val_data = CustomImageDataset(csv_file = '../data/data_val.csv',\n",
        "                                img_dir_path = '../data',\n",
        "                                resize=True,\n",
        "                                transform=train_transform)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating our DataLoader\n",
        "\n",
        "Below, we create our DataLoader. The purpose of doing this is to load our data in\n",
        "batches to fit and test our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_data, \n",
        "                              batch_size=1, \n",
        "                              shuffle=True)\n",
        "\n",
        "val_dataloader = DataLoader(val_data, \n",
        "                            batch_size=1, \n",
        "                            shuffle=True)\n",
        "\n",
        "test_dataloader = DataLoader(test_data, \n",
        "                             batch_size=1, \n",
        "                             shuffle=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomNeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        # inspire by Turing award winning LeCun, Bengio and Hinton's paper from 1998\n",
        "        # https://ieeexplore.ieee.org/document/726791 (cited more than 25,000 times!!!!!!!!!)\n",
        "        # code from https://blog.paperspace.com/writing-lenet5-from-scratch-in-python/ \n",
        "        self.LeNet = nn.Sequential(     \n",
        "            # convolutional layers            \n",
        "            nn.Sequential(                                            # FIRST LAYER: (INPUT LAYER)\n",
        "              nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),    # CONVOLUTION \n",
        "              nn.BatchNorm2d(6),\n",
        "              nn.ReLU(),\n",
        "              nn.MaxPool2d(kernel_size = 2, stride = 2)),             # POOLING\n",
        "            # nn.Sequential(                                            # SECOND LAYER: HIDDEN LAYER 1\n",
        "            #   nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),   # CONVOLUTION \n",
        "            #   nn.BatchNorm2d(16),\n",
        "            #   nn.ReLU(),\n",
        "            #   nn.MaxPool2d(kernel_size = 2, stride = 2)),             # POOLING\n",
        "            # fully connected layers\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(31974, 64),                                   # THIRD LAYER: LINEAR YEAR, HIDDEN LAYER 2\n",
        "            nn.ReLU(),                                                # HIDDEN LAYER's ACTIVATION FUNCION\n",
        "            # nn.Linear(64, 64),                                       # FOURTH LAYER: LINEAR YEAR, HIDDEN LAYER 3\n",
        "            # nn.ReLU(),                                                # HIDDEN LAYER's ACTIVATION FUNCION\n",
        "            # output layer\n",
        "            nn.Linear(64, 2)                                          # OUTPUT LAYER\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.LeNet(x)\n",
        "        return out"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model's performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = CustomNeuralNetwork()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To assess the prediction capacity of our model we compare the model's loss an classification power. The first one is defined as the accuracy of the model in terms of the predicted probabilities, while the second refers to how good the model classifies the actual labels (*e.g.*, normal lungs or lungs with pneumonia).\n",
        "\n",
        "The loss functions that we will use are:\n",
        "\n",
        "1. Binary cross-entropy:\n",
        "2. Cross-entropy:\n",
        "\n",
        "On the other hand, the accuracy measures we'll use are:\n",
        "\n",
        "1. Accuracy:\n",
        "2. Recall:\n",
        "3. Precision:\n",
        "4. F1:\n",
        "\n",
        "**Falta dar una pequenia explicacion de estas medidas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a loss function and optimizer:\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/stello/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/home/stello/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Train and validate the network\n",
        "EPOCHS = 1\n",
        "\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "for _ in range(EPOCHS):  # loop over the dataset multiple times\n",
        "    # TRAIN\n",
        "    # Make sure gradient tracking is on, and do a pass over the data\n",
        "    model.train()\n",
        "    running_loss_train = 0.0\n",
        "    accuracies_train = []\n",
        "    f1_scores_train = []\n",
        "    for i, data in enumerate(train_dataloader):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        \n",
        "        inputs = inputs.float()\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # keep track of the loss\n",
        "        running_loss_train += loss.item()\n",
        "        \n",
        "        # ALSO CALCULATE YOUR ACCURACY METRIC\n",
        "        predicted = predicted.detach().numpy()\n",
        "        labels = labels.detach().numpy()\n",
        "\n",
        "        accuracy = metrics.accuracy_score(labels, predicted)\n",
        "        accuracies_train.append(accuracy)\n",
        "\n",
        "        f1score = metrics.f1_score(labels, predicted)\n",
        "        f1_scores_train.append(f1score)\n",
        "\n",
        "    #AVERAGE TRAINING LOSS  \n",
        "    avg_train_loss = running_loss_train / (i + 1)     # i + 1 gives us the total number of batches in train dataloader\n",
        "    # CALCULATE AVERAGE ACCURACY METRIC\n",
        "    avg_train_acc = sum(accuracies_train)/len(accuracies_train)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accuracies.append(avg_train_acc)\n",
        "\n",
        "    #VALIDATE\n",
        "    # in the validation part, we don't want to keep track of the gradients \n",
        "    model.eval()\n",
        "    running_loss_val = 0.0\n",
        "    accuracies_val = []\n",
        "    f1_scores_val = []\n",
        "    for i, data in enumerate(val_dataloader):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        \n",
        "        inputs = inputs.float()\n",
        "\n",
        "        # val prediction\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        # keep track of the loss\n",
        "        running_loss_val += loss.item()\n",
        "\n",
        "        # ALSO CALCULATE YOUR ACCURACY METRIC\n",
        "        predicted = predicted.detach().numpy()\n",
        "        labels = labels.detach().numpy()\n",
        "\n",
        "        accuracy = metrics.accuracy_score(labels, predicted)\n",
        "        accuracies_val.append(accuracy)\n",
        "\n",
        "        f1score = metrics.f1_score(labels, predicted)\n",
        "        f1_scores_val.append(f1score)\n",
        "\n",
        "    # AVERAGE VALIDATION LOSS\n",
        "    avg_val_loss = running_loss_val / (i + 1)     # i + 1 gives us the total number of batches in train dataloader\n",
        "    # CALCULATE AVERAGE ACCURACY METRIC\n",
        "    avg_val_acc = sum(accuracies_val)/len(accuracies_val)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_accuracies.append(avg_val_acc)            \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.00013118529795137651]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1.0]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.7339097261428833]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.0]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (1x3143376 and 31974x64)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[40], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     12\u001b[0m \u001b[39m# val prediction\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     14\u001b[0m _, predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs, \u001b[39m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[39m# ALSO CALCULATE YOUR ACCURACY METRIC\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[0;32mIn[37], line 31\u001b[0m, in \u001b[0;36mCustomNeuralNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 31\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mLeNet(x)\n\u001b[1;32m     32\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x3143376 and 31974x64)"
          ]
        }
      ],
      "source": [
        "# Testing the model\n",
        "\n",
        "model.eval()\n",
        "running_loss_test = 0.0\n",
        "accuracies_test = []\n",
        "f1_scores_test = []\n",
        "for i, data in enumerate(test_dataloader):\n",
        "    # get the inputs; data is a list of [inputs, labels]\n",
        "    inputs, labels = data\n",
        "    inputs = inputs.float()\n",
        "\n",
        "    # val prediction\n",
        "    outputs = model(inputs)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    # ALSO CALCULATE YOUR ACCURACY METRIC\n",
        "    predicted = predicted.detach().numpy()\n",
        "    labels = labels.detach().numpy()\n",
        "\n",
        "    accuracy = metrics.accuracy_score(labels, predicted)\n",
        "    accuracies_test.append(accuracy)\n",
        "\n",
        "    f1score = metrics.f1_score(labels, predicted)\n",
        "    f1_scores_test.append(f1score)\n",
        "\n",
        "# AVERAGE VALIDATION LOSS\n",
        "avg_test_loss = running_loss_test / (i + 1)     # i + 1 gives us the total number of batches in train dataloader\n",
        "# CALCULATE AVERAGE ACCURACY METRIC\n",
        "avg_test_acc = sum(accuracies_test)/len(accuracies_test)   \n",
        "    \n",
        "    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
